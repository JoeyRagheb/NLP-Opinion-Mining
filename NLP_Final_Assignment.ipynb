{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment: Opinion Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project there are 2 approaches taken: Unsupervised and Supervised and extended further exploration with apriori algorithm. The steps taken are the following: 0) Manually look at data and find inconsistencies such as taggs with { instead of \\[ character, or [+] without number 1, or 1 '#' instead of 2. But in the grand scheme of the project these errors are either dealt with a workaround or are insignificant.  1) Set up documents needed and import all libraries needed. 2) Parse the data and plug result into data types with the best ways to be managed, i.e.: dataframes, dicts, and strings. 3)  PreProcess: Preprocess some data to obtain cashedwords. 4) Tokenize: Tokenise the words that are gathered. 5) Aspect Extraction: Retrieve the aspects from the reviews. 6) Opinion mining: Retrieve the top 5 features associated with the product and determine their orientation whether positive or negative. 7) Evaluate algorithm by using metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import libraries and read the files. Obtain the file directories as well as the product names from the the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer', 'Router', 'Speaker', 'Apex AD2600 Progressive-scan DVD player', 'Canon G3', 'Creative Labs Nomad Jukebox Zen Xtra 40GB', 'Nikon coolpix 4300', 'Nokia 6610', 'Canon PowerShot SD500', 'Canon S100', 'Diaper Champ', 'Hitachi router', 'ipod', 'Linksys Router', 'MicroMP3', 'Nokia 6600', 'norton']\n",
      "/Downloads/AI Masters/Bath/NLP/Week_8/Data\\CustomerReviews-3_domains\\Computer.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "# --\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet\n",
    "# ---\n",
    "import ast\n",
    "import collections\n",
    "from itertools import dropwhile\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "dirPath = glob.iglob('/Users/*/Downloads/AI Masters/Bath/NLP/Week_8/Data/*')\n",
    "\n",
    "product_list = []\n",
    "path_product_list = []\n",
    "\n",
    "#loop through files and get product names and file directory\n",
    "for big_file in dirPath:\n",
    "    files = os.listdir(big_file)\n",
    "    product_list.append(files)\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and file != \"Readme.txt\":\n",
    "            path_product_list.append((os.path.join(big_file, file)))\n",
    "            with open(os.path.join(big_file, file)) as fp:\n",
    "                line = fp.readline()\n",
    "\n",
    "product_list = [i for si in product_list for i in si]\n",
    "product_list = [x for x in product_list if \".txt\" in x and \"Readme.txt\" not in x]\n",
    "product_list = [s.replace(\".txt\", \"\") for s in product_list]\n",
    "print(product_list)\n",
    "print(path_product_list[0][17:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data from Whitespaces, split the features from the text. Insure spaces exist between special characters and the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Feature1  \\\n",
      "0   I purchased this monitor because of budgetary...                        \n",
      "1   This item was the most inexpensive 17 inch mo...   inexpensive[+1][a]   \n",
      "2   My overall experience with this monitor was v...          monitor[-1]   \n",
      "3   When the screen was n't contracting or glitch...  picture quality[-1]   \n",
      "4   I 've viewed numerous different monitor model...  picture quality[-1]   \n",
      "\n",
      "      Feature2 Feature3 Feature4 Feature5 Feature6 Feature7 Feature8 Feature9  \n",
      "0                                                                              \n",
      "1                                                                              \n",
      "2                                                                              \n",
      "3   screen[-1]                                                                 \n",
      "4  monitor[-1]                                                                 \n"
     ]
    }
   ],
   "source": [
    "def setup(fileindex):\n",
    "    text_list = []\n",
    "    line =''\n",
    "    #Read lines from file\n",
    "    with open(path_product_list[fileindex]) as fp:\n",
    "        lines = list(line for line in (l.strip() for l in fp) if line)\n",
    "        text_list = lines\n",
    "\n",
    "    # Create dataframe with splitting sentences at ##\n",
    "    global df\n",
    "    df = DataFrame(text_list, columns=['text'])\n",
    "\n",
    "    fool = lambda x: pd.Series([i for i in reversed(x.split('##'))])\n",
    "    rev0 = df['text'].apply(fool)\n",
    "    rev0.rename(columns={0: 'Text', 1: 'Features'}, inplace=True)\n",
    "    global feature_column\n",
    "    feature_column = rev0.copy()\n",
    "    # Create string of sentences without the features\n",
    "    text_string = df[\"text\"].tolist()\n",
    "    text_string = ' '.join(text_string)\n",
    "    text_string = text_string.replace('][', '] [')\n",
    "    text_string = text_string.replace('[', ' [')\n",
    "\n",
    "    for i in range(0, len(rev0)):\n",
    "        if pd.isna(rev0.iloc[i]['Features']):\n",
    "            rev0.at[i, 'Features'] = rev0.iloc[i]['Text']\n",
    "            rev0.at[i, 'Text'] = ''\n",
    "    #split features in dataframe\n",
    "    foo2 = lambda x: pd.Series([i for i in reversed(x.split(','))])\n",
    "    rev1 = rev0['Features'].apply(foo2)\n",
    "    rev1.insert(loc=0, column='Text', value=rev0['Text'])\n",
    "\n",
    "    for i in range(0, len(rev1.columns) - 1):\n",
    "        rev1[i] = rev1[i].str.strip()\n",
    "\n",
    "    for i in range(1, len(rev1.columns)):\n",
    "        rev1 = rev1.rename(columns={rev1.columns[i]: 'Feature' + str(i)})\n",
    "\n",
    "    rev1.fillna(\"\", inplace=True)\n",
    "\n",
    "    rev2 = rev1.iloc[:, 1:len(rev1.columns)].copy()\n",
    "    for i in range(0, len(rev2.columns)):\n",
    "        rev2 = rev2.rename(columns={rev2.columns[i]: 'Features_Concat'})\n",
    "\n",
    "    rev3 = rev2.iloc[:, 0:1].copy()\n",
    "    for i in range(0, len(rev2.columns) - 1):\n",
    "        rev3 = rev3.append(rev2.iloc[:, i:i + 1])\n",
    "\n",
    "    foo3 = lambda x: pd.Series([i for i in x.split('[')])\n",
    "    rev4 = rev3['Features_Concat'].apply(foo3)\n",
    "    return [rev1, text_string]\n",
    "\n",
    "[rev1, text_string] = setup(0)\n",
    "print(rev1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcess: use nltk to gather stopwords and print first 10. Additional words that can be excluded can be obtained through obtaining high frequency n-grams in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "def PreProcess():\n",
    "    cachedStopWords = nltk.corpus.stopwords.words('english')\n",
    "    cachedStopWords = cachedStopWords + ['im'] + ['i\\'m']\n",
    "    result = (' '.join([word for word in text_string.split() if word.lower() not in cachedStopWords]))\n",
    "    cStopWords = cachedStopWords\n",
    "    return cStopWords\n",
    "cachedStopWords = PreProcess()\n",
    "print(cachedStopWords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize: tokenize the words in the list and print first 5. Additional cleaning is performed later depending on need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purchased', 'monitor', 'budgetary', 'concerns', 'inexpensive']\n"
     ]
    }
   ],
   "source": [
    "def process(text):\n",
    "    stoplist = set(stopwords.words('english'))  # This is the PreProcessing\n",
    "    word_list = [word for word in word_tokenize(text.lower()) if\n",
    "                 word not in stoplist and word not in string.punctuation]  # Tokenization\n",
    "    return word_list\n",
    "processed_list = process(text_string)\n",
    "print(processed_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS Tagging: Use POS tagging on the full list of words. The reason for that is in order to be able to get opinions, the algorithm will look at the sentence as a whole and determine the orientation based on all the sentence. As well as having a dict for cleaned up words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean list:\n",
      "[(0, [('purchased', 'VBN'), ('monitor', 'NN'), ('budgetary', 'JJ'), ('concerns', 'NNS')]), (1, [('item', 'NN'), ('inexpensive', 'JJ'), ('17', 'CD'), ('inch', 'NN'), ('monitor', 'NN'), ('available', 'JJ'), ('time', 'NN'), ('made', 'VBD'), ('purchase', 'NN')])]\n",
      "Original List: POS Tagged\n",
      "[(0, [('I', 'PRP'), ('purchased', 'VBD'), ('this', 'DT'), ('monitor', 'NN'), ('because', 'IN'), ('of', 'IN'), ('budgetary', 'JJ'), ('concerns', 'NNS'), ('.', '.')]), (1, [('This', 'DT'), ('item', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('most', 'RBS'), ('inexpensive', 'JJ'), ('17', 'CD'), ('inch', 'NN'), ('monitor', 'NN'), ('available', 'JJ'), ('to', 'TO'), ('me', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('time', 'NN'), ('I', 'PRP'), ('made', 'VBD'), ('the', 'DT'), ('purchase', 'NN'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "def posTagging(rev1):\n",
    "    df_dict = rev1[\"Text\"].to_dict()\n",
    "    df_dict_clean_list = list(df_dict.items())\n",
    "    df_dict_clean_list = [(i, j.replace('[', ' [').replace('],', '] ,').replace('##', '').replace('][', '] [').replace('  ', ' '))\n",
    "                          for i, j in df_dict_clean_list]\n",
    "    df_dict_clean_list = [list(elem) for elem in df_dict_clean_list]\n",
    "    for i in range(0, len(df_dict_clean_list)):\n",
    "        df_dict_clean_list[i][1] = process(df_dict_clean_list[i][1])\n",
    "        df_dict_clean_list[i][1] = pos_tag(df_dict_clean_list[i][1])\n",
    "    df_dict_clean_list = [tuple(l) for l in df_dict_clean_list]\n",
    "    df_dict_clean_list = dict(df_dict_clean_list)\n",
    "    print('Clean list:')\n",
    "    print(list(df_dict_clean_list.items())[:2])\n",
    "\n",
    "    df_dict = rev1[\"Text\"].to_dict()\n",
    "    df_dict_list = list(df_dict.items())\n",
    "    df_dict_list = [(i, j.replace('[', ' [').replace('##', '').replace('][', '] [').replace('  ', ' ')) for i, j in df_dict_list]\n",
    "    df_dict_list = [list(elem) for elem in df_dict_list]\n",
    "    for i in range(0, len(df_dict_list)):\n",
    "        df_dict_list[i][1] = df_dict_list[i][1].split()\n",
    "        df_dict_list[i][1] = pos_tag(df_dict_list[i][1])\n",
    "    df_dict_list = [tuple(l) for l in df_dict_list]\n",
    "    df_dict_list = dict(df_dict_list)\n",
    "    print('Original List: POS Tagged')\n",
    "    print(list(df_dict_list.items())[:2])\n",
    "\n",
    "    return [df_dict_clean_list, df_dict_list]\n",
    "[df_dict_clean_list, df_dict_list] = posTagging(rev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect Extraction: In order to gather the aspects, the direction used was to loop through the sentences and words and group any nouns that occur after each other. That way we will be able to capture features such as: \"Picture quality\", thanks to the tags being 'NN' or 'NNP'. Create 1 list of tuples for feature at sentence index. Create second list of tuples for count of feature in all the sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 6 Aspect List tuple by indexed line: \n",
      "[('MONITOR', 1), ('ITEM', 1), ('TIME', 1), ('PURCHASE', 2), ('EXPERIENCE', 2), ('MONITOR', 3)]\n",
      "Aspects by count are: \n",
      "[('MONITOR', 72), ('PRICE', 28), ('ACER', 26), ('TIME', 24), ('COMPUTER', 22), ('SCREEN', 17), ('DISPLAY', 14), ('PROBLEM', 14), ('NETBOOK', 13), ('PRODUCT', 12), ('PURCHASE', 9), ('LAPTOP', 9), ('-RRB-', 9), ('CASE', 8), ('LOT', 8), ('ITEM', 7), ('KEYBOARD', 7), ('PICTURE', 7), ('AMAZON', 7), ('LCD', 7), ('MONEY', 7), ('DAY', 6), ('BRAND', 6), ('SIZE', 6), ('-LRB-', 6), ('SOMETHING', 6), ('REPLACEMENT', 6), ('IMAGE', 6), ('QUALITY', 6), ('THING', 6), ('UNIT', 6), ('DEAL', 6), ('SUPPORT', 6), ('EXPERIENCE', 5), ('DELL', 5), ('USER', 5), ('MACHINE', 5), ('CONTRAST', 5), ('BRIGHTNESS', 5), ('RESOLUTION', 5), ('SYSTEM', 5), ('DESK', 5), ('DIFFERENCE', 5), ('APPLE', 5), ('RAM', 4), ('DEVICE', 4), ('OPINION', 4), ('YEAR', 4), ('ANYONE', 4), ('REVIEW', 4), ('COLOR', 4), ('LCD MONITOR', 4), ('LIFE', 4), ('WEIGHT', 4), ('ANYTHING', 4), ('WAY', 4), ('REPAIR', 4), ('CUSTOMER SERVICE', 4), ('INTERNET', 4), ('WORK', 4), ('EVERYTHING', 4), ('PC', 4), ('INFORMATION', 4), ('IDEA', 4), ('POTPLAYER', 4), ('MOVIE', 4), ('WEEK', 3), ('BOX', 3), ('MEMORY', 3), ('CARE', 3), ('TABLET', 3), ('WHILE', 3), ('CONDITION', 3), ('SCREEN MONITOR', 3), ('SAMSUNG', 3), ('FACT', 3), ('MONTH', 3), ('BATTERY LIFE', 3), ('DESIGN', 3), ('GRIPE', 3), ('END', 3), ('WARRANTY', 3), ('WINDOWS', 3), ('HOME', 3), ('DESKTOP', 3), ('MINE', 3), ('SPACE', 3), ('EMAIL', 3), ('SHIPPING', 3), ('VALUE', 3), ('GPU HARDWARE ACCELERATION', 3), ('VIEWING', 3), ('BUTTON', 3), ('MINI', 3), ('PICTURE QUALITY', 2), ('TEXT', 2), ('DIFFICULTY', 2), ('NETWORK', 2), ('SCREEN QUALITY', 2), ('PHOTOSHOP', 2), ('GEAR', 2), ('CARD', 2), ('CONNECTION', 2), ('PRO', 2), ('BATCH', 2), ('SOMEONE', 2), ('REASON', 2), ('PERFORMANCE', 2), ('PERSON', 2), ('NOTEBOOK', 2), ('BUSINESS', 2), ('ACER MONITOR', 2), ('PRICESS', 2), ('SPECS', 2), ('PACKAGE', 2), ('VGA CONNECTION', 2), ('PIECE', 2), ('DVI INPUT', 2), ('ADDITION', 2), ('NOTE', 2), ('VIDEO', 2), ('LUCK', 2), ('VIEWSONIC', 2), ('BACKLIGHT', 2), ('NOTHING', 2), ('QUESTION', 2), ('BATTERY', 2), ('!!!', 2), ('SOUND', 2), ('COST', 2), ('CORNER', 2), ('CRISP', 2), ('BEAT', 2), ('CLARITY', 2), ('OFFICE', 2), ('PLASTIC', 2), ('AREA', 2), ('PERFECT', 2), ('ONE', 2), ('ISSUE', 2), ('HALF', 2), ('IM', 2), ('AMOUNT', 2), ('DEFAULT', 2), ('BUY', 2), ('DELIVERY', 2), ('MATTE', 2), ('NIGHT', 2), ('RESULT', 2), ('LG', 2), ('HD', 2), ('UPGRADE', 2), ('STAND', 2), ('MODEL', 2), ('PRESS', 2), ('KEY', 2), ('CHARACTER', 2), ('SPEC', 2), ('BARGAIN', 2), ('CHRISTMAS', 2), ('ROOM', 2), ('SERVICE', 2), ('ACER PRODUCT', 2), ('ANGLE', 2), ('TARGET', 2), ('PLAYER', 2), ('DROP', 2), ('GIG', 2), ('BEEP', 2), ('MAKE SURE', 2), ('GADGET', 2), ('WEATHER', 2), ('WALL', 2), ('GLANCE', 2), ('TEMPERATURE', 2), ('AMBIENT', 2), ('PANEL', 2), ('INSTALLATION', 2), ('AOC', 2), ('OS X', 2), ('FIRMWARE', 2), ('REFLECTION', 2), ('POWER', 2)]\n"
     ]
    }
   ],
   "source": [
    "def AspectExtraction(df_dict_list):\n",
    "    prevWord = ''\n",
    "    prevTag = ''\n",
    "    currWord = ''\n",
    "    global aspectListtuple\n",
    "    aspectListtuple = []\n",
    "    aspectList = []\n",
    "    outDict = {}\n",
    "\n",
    "    # Extracting Aspects\n",
    "    # Add words together if they are nouns\n",
    "    for key, value in df_dict_list.items():\n",
    "        for word, tag in value:\n",
    "            if (tag == 'NN' or tag == 'NNP'):\n",
    "                if (prevTag == 'NN' or prevTag == 'NNP'):\n",
    "                    if (prevWord == word):\n",
    "                        currWord = word\n",
    "                    else:\n",
    "                        currWord = prevWord + ' ' + word\n",
    "                else:\n",
    "                    aspectList.append(prevWord.upper())\n",
    "                    aspectListtuple = aspectListtuple + [[prevWord.upper()] + [key]]\n",
    "                    currWord = word\n",
    "            prevWord = currWord\n",
    "            prevTag = tag\n",
    "\n",
    "    #Clean list\n",
    "    for i in range(0,len(aspectListtuple)):\n",
    "        aspectListtuple[i][0] = [s.replace(\" [\", \"[\").replace(\" [+1]\", \"\").replace(\" [-1]\", \"\").replace(\"[+1]\", \"\").replace(\"[-1]\", \"\").replace(\"[A]\", \"\").replace(\"[V]\", \"\").replace(\n",
    "                \"[T]\", \"\").replace(\",\", \"\") for s in [aspectListtuple[i][0]]][0]\n",
    "    #Create aspect list tuple of aspect and line it appears in\n",
    "    aspectListtuple = [tuple(l) for l in aspectListtuple]\n",
    "    temp = []\n",
    "    for i in range(0, len(aspectListtuple)):\n",
    "        if list(aspectListtuple[i])[0] != \"\":\n",
    "            temp += [tuple(aspectListtuple[i])]\n",
    "    aspectListtuple = temp\n",
    "    temp =[]\n",
    "    count_dict = collections.Counter([x for (x,y) in aspectListtuple])\n",
    "    for key, countii in count_dict.items():\n",
    "        if countii > 1:\n",
    "            temp += [(key, countii)]\n",
    "    count_dict = temp\n",
    "    temp = []\n",
    "    for aspect,c in aspectListtuple:\n",
    "        if aspect in [item for sublist in [list(elem) for elem in count_dict] for item in sublist]:\n",
    "            temp += [(aspect,c)]\n",
    "    aspectListtuple = temp\n",
    "    print('First 6 Aspect List tuple by indexed line: ')\n",
    "    print(aspectListtuple[:6])\n",
    "\n",
    "    # Clean aspect List\n",
    "    aspectList2 = []\n",
    "    for i in range(0, len(aspectList)):\n",
    "        if ' [' in aspectList[i]:\n",
    "            temp1 = aspectList[i][aspectList[i].index(\" [\"):]\n",
    "            temp2 = aspectList[i][:aspectList[i].index(\" [\")]\n",
    "            aspectList2.append(temp1)\n",
    "            aspectList2.append(temp2)\n",
    "        else:\n",
    "            aspectList2.append(aspectList[i])\n",
    "\n",
    "    aspectList2 = [\n",
    "        s.replace(\" [\", \"[\").replace(\"[+1]\", \"\").replace(\"[-1]\", \"\").replace(\"[A]\", \"\").replace(\"[V]\", \"\").replace(\n",
    "            \"[T]\", \"\").replace(\",\", \"\") for s in aspectList2]\n",
    "    aspectList2 = list(filter(None, aspectList2))\n",
    "    aspectList = aspectList2\n",
    "    # Eliminating aspect which has 1 or less count\n",
    "    for aspect in aspectList:\n",
    "        if (aspectList.count(aspect) > 1):\n",
    "            if (outDict.keys() != aspect):\n",
    "                outDict[aspect] = aspectList.count(aspect)\n",
    "    outputAspect = sorted(outDict.items(), key=lambda x: x[1], reverse=True)\n",
    "    outputAspect = [i for i in outputAspect if i[0] != '##']\n",
    "    outputAspect = [i for i in outputAspect if len(i[0]) > 1]\n",
    "    print('Aspects by count are: ')\n",
    "    print(outputAspect)\n",
    "    return outputAspect\n",
    "outputAspect = AspectExtraction(df_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opinion Mining: Determine orientation of word using wordnet.synsets and sentiwordnet.senti_synset. Setup negativeWordSet for which if a sentence starts off positive and later on has a 'but' in the sentence then that will be an indication that it should be now considered negative. Then, setup which sentences at what index are positive or negative based on orientation. Print top 5 features based on count with their associated positive and negative counts. Rather than using the orientation method below a more straightforward way to do it would be to use sentimentanalyser() but this approach was preferred to explore the reasonning behind sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Computer\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\CustomerReviews-3_domains\\Computer.txt\n",
      "MONITOR :\t\tPositive ==>  34 :\t\tNegative ==>  12\n",
      "PRICE :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "TIME :\t\tPositive ==>  4 :\t\tNegative ==>  0\n",
      "PURCHASE :\t\tPositive ==>  4 :\t\tNegative ==>  1\n",
      "PRO :\t\tPositive ==>  4 :\t\tNegative ==>  0\n"
     ]
    }
   ],
   "source": [
    "def orientation(inputWord):\n",
    "    wordSynset = wordnet.synsets(inputWord)\n",
    "    if (len(wordSynset) != 0):\n",
    "        word = wordSynset[0].name()\n",
    "        orientation = sentiwordnet.senti_synset(word)\n",
    "        if (orientation.pos_score() > orientation.neg_score()):\n",
    "            return True\n",
    "        elif (orientation.pos_score() < orientation.neg_score()):\n",
    "            return False\n",
    "\n",
    "\n",
    "def OpinionMining(outputAspect, df_dict_list,n):\n",
    "    oAspectOpinionTuples = {}\n",
    "    JOpinionList = []\n",
    "    orientationCache = {}\n",
    "    #Switch orientation based on below words\n",
    "    negativeWordSet = {\"don't\", \"never\", \"nothing\", \"nowhere\", \"noone\", \"none\", \"not\",\n",
    "                       \"hasn't\", \"hadn't\", \"can't\", \"couldn't\", \"shouldn't\", \"won't\",\n",
    "                       \"wouldn't\", \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"ain't\"}\n",
    "    #Determine orientation and switch if applicable\n",
    "    out = [\"\"]\n",
    "    for aspect, no in outputAspect:\n",
    "        aspectTokens = word_tokenize(aspect)\n",
    "        count = 0\n",
    "        for key, value in df_dict_list.items():\n",
    "            condition = True\n",
    "            isNegativeSen = False\n",
    "            for subWord in aspectTokens:\n",
    "                if (subWord in str(value).upper()):\n",
    "                    condition = condition and True\n",
    "                else:\n",
    "                    condition = condition and False\n",
    "            # Switch orientation if word in negativeWordSet \n",
    "            if (condition):\n",
    "                for negWord in negativeWordSet:\n",
    "                    if (not isNegativeSen):\n",
    "                        if negWord.upper() in str(value).upper():\n",
    "                            isNegativeSen = isNegativeSen or True\n",
    "\n",
    "                oAspectOpinionTuples.setdefault(aspect, [0, 0, 0])\n",
    "                #Use words that will make a difference in opinion\n",
    "                for word, tag in value:\n",
    "\n",
    "                    if (tag == 'JJ' or tag == 'JJR' or tag == 'JJS' or tag == 'RB' or tag == 'RBR' or tag == 'RBS'):\n",
    "                        count += 1\n",
    "                        if (word not in orientationCache):\n",
    "                            orien = orientation(word)\n",
    "                            orientationCache[word] = orien\n",
    "                        else:\n",
    "                            orien = orientationCache[word]\n",
    "                        if (isNegativeSen and orien is not None):\n",
    "                            orien = not orien\n",
    "                        JOpinionList = JOpinionList + [(key, orien)]\n",
    "        JOpinionList_ = []\n",
    "        for i in range(0, len(JOpinionList)):\n",
    "            if i != len(JOpinionList) - 1:\n",
    "                if JOpinionList[i][0] != JOpinionList[i + 1][0]:\n",
    "                    JOpinionList_ = JOpinionList_ + [JOpinionList[i]]\n",
    "            else:\n",
    "                JOpinionList_ = JOpinionList_ + [JOpinionList[i]]\n",
    "        positives = []\n",
    "        negatives = []\n",
    "        Neutral = []\n",
    "        pcount = 0\n",
    "        necount = 0\n",
    "        nucount = 0\n",
    "        # Setup which sentences at what index are positive or negative based on orientation\n",
    "        for aspect, no in outputAspect:\n",
    "            for key, Sentiment in JOpinionList_:\n",
    "                # And JOpinionList_ key is true\n",
    "                if ' '.join([str(elem) for elem in df_dict_clean_list.get(key)]).find(aspect.lower()) >= 0:\n",
    "                    if Sentiment == True:\n",
    "                        pcount += 1\n",
    "                    if Sentiment == False:\n",
    "                        necount += 1\n",
    "                    if Sentiment == None:\n",
    "                        nucount += 1\n",
    "            positives = positives + [(aspect, pcount)]\n",
    "            negatives = negatives + [(aspect, necount)]\n",
    "            Neutral = Neutral + [(aspect, nucount)]\n",
    "            pcount = 0\n",
    "            necount = 0\n",
    "            nucount = 0\n",
    "        aspect_reviews = []\n",
    "        for i in range(0, len(positives)):\n",
    "            aspect_reviews = aspect_reviews + [list(positives[i]) + [negatives[i][1]] + [Neutral[i][1]]]\n",
    "        aspect_reviews_c = []\n",
    "        for i in range(0, len(aspect_reviews)):\n",
    "            if aspect_reviews[i][0].lower() not in cachedStopWords and len(aspect_reviews[i][0].lower()) > 1:\n",
    "                aspect_reviews_c = aspect_reviews_c + [aspect_reviews[i]]\n",
    "        saspect_reviews = sorted(aspect_reviews_c, key=lambda x: x[1], reverse=True)[:5]\n",
    "        print('Product: ' + str(product_list[n]))\n",
    "        print('From path: ' + str(path_product_list[n][17:]))\n",
    "        for i in range(0, len(saspect_reviews)):\n",
    "            print(saspect_reviews[i][0], ':\\t\\tPositive ==> ', saspect_reviews[i][1], ':\\t\\tNegative ==> ',\n",
    "                  saspect_reviews[i][2])\n",
    "        return\n",
    "TopFiveReviews = OpinionMining(outputAspect, df_dict_list,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate metrics of the algorithm by calculating its accuracy. Calculate accuracy solely based on exact word to word comparison.  Then set a threshold for wup_similarity and calculate similarity of related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision word to word is: 0.2909604519774011\n",
      "Recall word to word is 0.25183374083129584\n",
      "Precision for related words using wup_similarity is: 0.7799511002444988\n",
      "Recall wup_similarity is 0.9011299435028248\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "def metrics():\n",
    "    #Clean up\n",
    "    remove_from_features = ['[+1]','[-1]','[A]','[V]','[a]','[t]','[v]']\n",
    "    for i in remove_from_features:\n",
    "        feature_column['Features'] = feature_column['Features'].str.replace(i, '', regex=False)\n",
    "    features_at_lines = list(feature_column['Features'].items())\n",
    "    counti = 0\n",
    "    temp = []\n",
    "    features_at_lines = [(sub[1], sub[0]) for sub in features_at_lines]\n",
    "    for i in features_at_lines:\n",
    "        if ',' in i[0]:\n",
    "            spliti = i[0].split(',')\n",
    "            for ele in spliti:\n",
    "                temp += [(ele.upper().strip(), i[1])]\n",
    "        else:\n",
    "             temp += [(i[0].upper().strip(),i[1])]\n",
    "    features_at_lines = temp\n",
    "    #Get features at lines to compare to aspects retrieved at line\n",
    "    features_at_lines_thathavetags = []\n",
    "    aspectListtuple_thathavetags = []\n",
    "    lines_without_tags = []\n",
    "    for i in features_at_lines:\n",
    "        if i[0] == '':\n",
    "            lines_without_tags += [i[1]]\n",
    "        else:\n",
    "            features_at_lines_thathavetags += [i]\n",
    "\n",
    "    for i in aspectListtuple:\n",
    "        if i[1] not in lines_without_tags:\n",
    "            aspectListtuple_thathavetags += [i]\n",
    "\n",
    "    mining_aspects = aspectListtuple_thathavetags\n",
    "    manual_aspects = features_at_lines_thathavetags\n",
    "    \n",
    "    #Calculate word for word accuracy\n",
    "    true_positive = 0\n",
    "    for i in manual_aspects:\n",
    "        if i in mining_aspects:\n",
    "            true_positive += 1\n",
    "    #true_positive + false_positive = manual counts\n",
    "    true_positive_and_false_positive = len(manual_aspects)\n",
    "    precision = true_positive/true_positive_and_false_positive\n",
    "    print(\"Precision word to word is: \" + str(precision))\n",
    "    true_positive_and_false_negative = len(mining_aspects)\n",
    "    recall1 = true_positive/true_positive_and_false_negative\n",
    "    print(\"Recall word to word is \" + str(recall1))\n",
    "    \n",
    "    #Use wup_similarity for related words accuracy\n",
    "    true_positive_related1 = 0\n",
    "    for i in range(0,len(manual_aspects)):\n",
    "        for ii in range(0,len(mining_aspects)):\n",
    "            if manual_aspects[i][1] == mining_aspects[ii][1]:\n",
    "                wordx, wordy = manual_aspects[i][0],mining_aspects[ii][0]\n",
    "                if wordx == wordy:\n",
    "                    true_positive_related1 += 1\n",
    "                else:\n",
    "                    sem1, sem2 = wn.synsets(wordx), wn.synsets(wordy)\n",
    "                    maxscore = 0\n",
    "                    for il, j in list(product(*[sem1, sem2])):\n",
    "                        score = il.wup_similarity(j)\n",
    "                        if score != None:\n",
    "                            maxscore = score if maxscore < score else maxscore\n",
    "                    if maxscore > 0.4:\n",
    "                        true_positive_related1 += 1\n",
    "    true_positive_and_false_positive_related1 = len(mining_aspects)\n",
    "    precision2 = true_positive_related1 / true_positive_and_false_positive_related1\n",
    "    print(\"Precision for related words using wup_similarity is: \" + str(precision2))\n",
    "    true_positive_and_false_negative2 = len(manual_aspects)\n",
    "    recall2 = true_positive_related1/true_positive_and_false_negative2\n",
    "    print(\"Recall wup_similarity is \" + str(recall2))\n",
    "metrics = metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running through all files (Code is copied from above to loop through all files)\n",
    "#### Cell below is only used to print output\n",
    "The code below will loop through all the files and grab the features and print the top five features with their positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Computer\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\CustomerReviews-3_domains\\Computer.txt\n",
      "MONITOR :\t\tPositive ==>  34 :\t\tNegative ==>  12\n",
      "PRICE :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "TIME :\t\tPositive ==>  4 :\t\tNegative ==>  0\n",
      "PURCHASE :\t\tPositive ==>  4 :\t\tNegative ==>  1\n",
      "PRO :\t\tPositive ==>  4 :\t\tNegative ==>  0\n",
      "Product: Router\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\CustomerReviews-3_domains\\Router.txt\n",
      "ROUTER :\t\tPositive ==>  46 :\t\tNegative ==>  24\n",
      "WORK :\t\tPositive ==>  8 :\t\tNegative ==>  5\n",
      "ONE :\t\tPositive ==>  8 :\t\tNegative ==>  6\n",
      "-RRB- :\t\tPositive ==>  6 :\t\tNegative ==>  3\n",
      "-LRB- :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "Product: Speaker\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\CustomerReviews-3_domains\\Speaker.txt\n",
      "SOUND :\t\tPositive ==>  45 :\t\tNegative ==>  18\n",
      "SPEAKER :\t\tPositive ==>  13 :\t\tNegative ==>  5\n",
      "QUALITY :\t\tPositive ==>  9 :\t\tNegative ==>  3\n",
      "BASS :\t\tPositive ==>  6 :\t\tNegative ==>  0\n",
      "-RRB- :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "Product: Apex AD2600 Progressive-scan DVD player\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Customer_review_data\\Apex AD2600 Progressive-scan DVD player.txt\n",
      "PLAYER :\t\tPositive ==>  37 :\t\tNegative ==>  17\n",
      "DVD :\t\tPositive ==>  24 :\t\tNegative ==>  11\n",
      "APEX :\t\tPositive ==>  5 :\t\tNegative ==>  3\n",
      "ONE :\t\tPositive ==>  5 :\t\tNegative ==>  2\n",
      "DVDS :\t\tPositive ==>  4 :\t\tNegative ==>  0\n",
      "Product: Canon G3\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Customer_review_data\\Canon G3.txt\n",
      "CAMERA :\t\tPositive ==>  64 :\t\tNegative ==>  15\n",
      "CAMERAS :\t\tPositive ==>  11 :\t\tNegative ==>  5\n",
      "USE :\t\tPositive ==>  10 :\t\tNegative ==>  2\n",
      "MP :\t\tPositive ==>  10 :\t\tNegative ==>  4\n",
      "VIEW :\t\tPositive ==>  8 :\t\tNegative ==>  1\n",
      "Product: Creative Labs Nomad Jukebox Zen Xtra 40GB\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Customer_review_data\\Creative Labs Nomad Jukebox Zen Xtra 40GB.txt\n",
      "PLAYER :\t\tPositive ==>  66 :\t\tNegative ==>  44\n",
      "PLAY :\t\tPositive ==>  66 :\t\tNegative ==>  44\n",
      "MP3 :\t\tPositive ==>  12 :\t\tNegative ==>  14\n",
      "ONE :\t\tPositive ==>  9 :\t\tNegative ==>  6\n",
      "BATTERY :\t\tPositive ==>  7 :\t\tNegative ==>  0\n",
      "Product: Nikon coolpix 4300\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Customer_review_data\\Nikon coolpix 4300.txt\n",
      "CAMERA :\t\tPositive ==>  33 :\t\tNegative ==>  20\n",
      "MP :\t\tPositive ==>  5 :\t\tNegative ==>  5\n",
      "PICTURE :\t\tPositive ==>  4 :\t\tNegative ==>  4\n",
      "ONE :\t\tPositive ==>  4 :\t\tNegative ==>  4\n",
      "DIGITAL :\t\tPositive ==>  4 :\t\tNegative ==>  2\n",
      "Product: Nokia 6610\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Customer_review_data\\Nokia 6610.txt\n",
      "PHONE :\t\tPositive ==>  62 :\t\tNegative ==>  37\n",
      "NOKIA :\t\tPositive ==>  10 :\t\tNegative ==>  2\n",
      "USE :\t\tPositive ==>  9 :\t\tNegative ==>  5\n",
      "CELL :\t\tPositive ==>  9 :\t\tNegative ==>  1\n",
      "SPEAKERPHONE :\t\tPositive ==>  7 :\t\tNegative ==>  0\n",
      "Product: Canon PowerShot SD500\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Canon PowerShot SD500.txt\n",
      "CAMERA :\t\tPositive ==>  22 :\t\tNegative ==>  14\n",
      "TIME :\t\tPositive ==>  3 :\t\tNegative ==>  2\n",
      "QUALITY :\t\tPositive ==>  2 :\t\tNegative ==>  3\n",
      "LIGHT :\t\tPositive ==>  2 :\t\tNegative ==>  1\n",
      "CANON :\t\tPositive ==>  2 :\t\tNegative ==>  0\n",
      "Product: Canon S100\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Canon S100.txt\n",
      "CAMERA :\t\tPositive ==>  29 :\t\tNegative ==>  15\n",
      "CAMERAS :\t\tPositive ==>  6 :\t\tNegative ==>  3\n",
      "PICTURE :\t\tPositive ==>  6 :\t\tNegative ==>  1\n",
      "PICTURES :\t\tPositive ==>  5 :\t\tNegative ==>  1\n",
      "SMALL :\t\tPositive ==>  4 :\t\tNegative ==>  3\n",
      "Product: Diaper Champ\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Diaper Champ.txt\n",
      "CHAMP :\t\tPositive ==>  14 :\t\tNegative ==>  8\n",
      "DIAPER :\t\tPositive ==>  14 :\t\tNegative ==>  8\n",
      "BAG :\t\tPositive ==>  6 :\t\tNegative ==>  1\n",
      "PAIL :\t\tPositive ==>  3 :\t\tNegative ==>  0\n",
      "BAGS :\t\tPositive ==>  3 :\t\tNegative ==>  1\n",
      "Product: Hitachi router\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Hitachi router.txt\n",
      "ROUTER :\t\tPositive ==>  26 :\t\tNegative ==>  13\n",
      "TABLE :\t\tPositive ==>  9 :\t\tNegative ==>  3\n",
      "M12 :\t\tPositive ==>  5 :\t\tNegative ==>  1\n",
      "M12V :\t\tPositive ==>  4 :\t\tNegative ==>  1\n",
      "ONE :\t\tPositive ==>  4 :\t\tNegative ==>  4\n",
      "Product: ipod\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\ipod.txt\n",
      "IPOD :\t\tPositive ==>  36 :\t\tNegative ==>  20\n",
      "MP3 :\t\tPositive ==>  7 :\t\tNegative ==>  2\n",
      "GB :\t\tPositive ==>  7 :\t\tNegative ==>  1\n",
      "PLAY :\t\tPositive ==>  7 :\t\tNegative ==>  3\n",
      "THING :\t\tPositive ==>  6 :\t\tNegative ==>  3\n",
      "Product: Linksys Router\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Linksys Router.txt\n",
      "ROUTER :\t\tPositive ==>  32 :\t\tNegative ==>  16\n",
      "LINK :\t\tPositive ==>  9 :\t\tNegative ==>  3\n",
      "LINKSYS :\t\tPositive ==>  7 :\t\tNegative ==>  2\n",
      "WRT54G :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "USE :\t\tPositive ==>  6 :\t\tNegative ==>  2\n",
      "Product: MicroMP3\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\MicroMP3.txt\n",
      "PLAYER :\t\tPositive ==>  41 :\t\tNegative ==>  25\n",
      "PLAY :\t\tPositive ==>  41 :\t\tNegative ==>  25\n",
      "ZEN :\t\tPositive ==>  11 :\t\tNegative ==>  3\n",
      "MP3 :\t\tPositive ==>  10 :\t\tNegative ==>  5\n",
      "USE :\t\tPositive ==>  9 :\t\tNegative ==>  2\n",
      "Product: Nokia 6600\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\Nokia 6600.txt\n",
      "PHONE :\t\tPositive ==>  52 :\t\tNegative ==>  19\n",
      "ONE :\t\tPositive ==>  52 :\t\tNegative ==>  19\n",
      "NOKIA :\t\tPositive ==>  10 :\t\tNegative ==>  6\n",
      "USE :\t\tPositive ==>  9 :\t\tNegative ==>  4\n",
      "CAMERA :\t\tPositive ==>  6 :\t\tNegative ==>  1\n",
      "Product: norton\n",
      "From path: /Downloads/AI Masters/Bath/NLP/Week_8/Data\\Reviews-9-products\\norton.txt\n",
      "NORTON :\t\tPositive ==>  13 :\t\tNegative ==>  7\n",
      "WORK :\t\tPositive ==>  3 :\t\tNegative ==>  0\n",
      "SECURITY :\t\tPositive ==>  3 :\t\tNegative ==>  1\n",
      "VERSION :\t\tPositive ==>  2 :\t\tNegative ==>  2\n",
      "PRODUCT :\t\tPositive ==>  2 :\t\tNegative ==>  3\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------Set up Data-------------------\n",
    "def setup(fileindex):\n",
    "    text_list = []\n",
    "    line =''\n",
    "    with open(path_product_list[fileindex]) as fp:\n",
    "        lines = list(line for line in (l.strip() for l in fp) if line)\n",
    "        text_list = lines\n",
    "\n",
    "\n",
    "    global df\n",
    "    df = DataFrame(text_list, columns=['text'])\n",
    "\n",
    "    fool = lambda x: pd.Series([i for i in reversed(x.split('##'))])\n",
    "    rev0 = df['text'].apply(fool)\n",
    "    rev0.rename(columns={0: 'Text', 1: 'Features'}, inplace=True)\n",
    "    global feature_column\n",
    "    feature_column = rev0.copy()\n",
    "\n",
    "    text_string = df[\"text\"].tolist()\n",
    "    text_string = ' '.join(text_string)\n",
    "    text_string = text_string.replace('][', '] [')\n",
    "    text_string = text_string.replace('[', ' [')\n",
    "\n",
    "    for i in range(0, len(rev0)):\n",
    "        if pd.isna(rev0.iloc[i]['Features']):\n",
    "            rev0.at[i, 'Features'] = rev0.iloc[i]['Text']\n",
    "            rev0.at[i, 'Text'] = ''\n",
    "\n",
    "    foo2 = lambda x: pd.Series([i for i in reversed(x.split(','))])\n",
    "    rev1 = rev0['Features'].apply(foo2)\n",
    "    rev1.insert(loc=0, column='Text', value=rev0['Text'])\n",
    "\n",
    "    for i in range(0, len(rev1.columns) - 1):\n",
    "        rev1[i] = rev1[i].str.strip()\n",
    "\n",
    "    for i in range(1, len(rev1.columns)):\n",
    "        rev1 = rev1.rename(columns={rev1.columns[i]: 'Feature' + str(i)})\n",
    "\n",
    "    rev1.fillna(\"\", inplace=True)\n",
    "\n",
    "    rev2 = rev1.iloc[:, 1:len(rev1.columns)].copy()\n",
    "    for i in range(0, len(rev2.columns)):\n",
    "        rev2 = rev2.rename(columns={rev2.columns[i]: 'Features_Concat'})\n",
    "\n",
    "    rev3 = rev2.iloc[:, 0:1].copy()\n",
    "    for i in range(0, len(rev2.columns) - 1):\n",
    "        rev3 = rev3.append(rev2.iloc[:, i:i + 1])\n",
    "\n",
    "    foo3 = lambda x: pd.Series([i for i in x.split('[')])\n",
    "    rev4 = rev3['Features_Concat'].apply(foo3)\n",
    "    return [rev1, text_string]\n",
    "\n",
    "\n",
    "# --------------PreProcessing------------------\n",
    "def PreProcess():\n",
    "    cachedStopWords = nltk.corpus.stopwords.words('english')\n",
    "    cachedStopWords = cachedStopWords + ['im'] + ['i\\'m']\n",
    "    result = (' '.join([WordNetLemmatizer().lemmatize(word.lower()) for word in text_string.split() if word.lower() not in cachedStopWords]))\n",
    "    cStopWords = cachedStopWords\n",
    "    return cStopWords\n",
    "\n",
    "\n",
    "# --------------PreProcessing------------------\n",
    "\n",
    "# --------------Tokenize------------------\n",
    "def process(text):\n",
    "    stoplist = set(stopwords.words('english'))  # This is the PreProcessing\n",
    "    word_list = [word for word in word_tokenize(text.lower()) if\n",
    "                 word not in stoplist and word not in string.punctuation]  # Tokenization\n",
    "    return word_list\n",
    "\n",
    "\n",
    "# --------------Tokenize------------------\n",
    "\n",
    "# --------------PoS Tagging------------------\n",
    "def posTagging(rev1):\n",
    "    df_dict = rev1[\"Text\"].to_dict()\n",
    "    df_dict_clean_list = list(df_dict.items())\n",
    "    df_dict_clean_list = [(i, j.replace('[', ' [').replace('],', '] ,').replace('##', '').replace('][', '] [').replace('  ', ' '))\n",
    "                          for i, j in df_dict_clean_list]\n",
    "    df_dict_clean_list = [list(elem) for elem in df_dict_clean_list]\n",
    "    for i in range(0, len(df_dict_clean_list)):\n",
    "        df_dict_clean_list[i][1] = process(df_dict_clean_list[i][1])\n",
    "        df_dict_clean_list[i][1] = pos_tag(df_dict_clean_list[i][1])\n",
    "    df_dict_clean_list = [tuple(l) for l in df_dict_clean_list]\n",
    "    df_dict_clean_list = dict(df_dict_clean_list)\n",
    "    # print(list(df_dict_clean_list.items())[:2])\n",
    "\n",
    "    df_dict = rev1[\"Text\"].to_dict()\n",
    "    df_dict_list = list(df_dict.items())\n",
    "    df_dict_list = [(i, j.replace('[', ' [').replace('##', '').replace('][', '] [').replace('  ', ' ')) for i, j in df_dict_list]\n",
    "    df_dict_list = [list(elem) for elem in df_dict_list]\n",
    "    for i in range(0, len(df_dict_list)):\n",
    "        df_dict_list[i][1] = df_dict_list[i][1].split()\n",
    "        df_dict_list[i][1] = pos_tag(df_dict_list[i][1])\n",
    "    df_dict_list = [tuple(l) for l in df_dict_list]\n",
    "    df_dict_list = dict(df_dict_list)\n",
    "    # print(list(df_dict_list.items())[:2])\n",
    "\n",
    "    return [df_dict_clean_list, df_dict_list]\n",
    "\n",
    "\n",
    "# --------------PoS Tagging------------------\n",
    "\n",
    "# --------------Aspect Extraction------------------\n",
    "def AspectExtraction(df_dict_list):\n",
    "    prevWord = ''\n",
    "    prevTag = ''\n",
    "    currWord = ''\n",
    "    global aspectListtuple\n",
    "    aspectListtuple = []\n",
    "    aspectList = []\n",
    "    outDict = {}\n",
    "\n",
    "    # Extracting Aspects\n",
    "    for key, value in df_dict_list.items():\n",
    "        for word, tag in value:\n",
    "            if (tag == 'NN' or tag == 'NNP'):\n",
    "                if (prevTag == 'NN' or prevTag == 'NNP'):\n",
    "                    if (prevWord == word):\n",
    "                        currWord = word\n",
    "                    else:\n",
    "                        currWord = prevWord + ' ' + word\n",
    "                        # aspectListtuple = aspectListtuple + [[currWord.upper()] + [key]]\n",
    "                else:\n",
    "                    aspectList.append(prevWord.upper())\n",
    "                    aspectListtuple = aspectListtuple + [[prevWord.upper()] + [key]]\n",
    "                    currWord = word\n",
    "            prevWord = currWord\n",
    "            prevTag = tag\n",
    "\n",
    "    for i in range(0,len(aspectListtuple)):\n",
    "        aspectListtuple[i][0] = [s.replace(\" [\", \"[\").replace(\" [+1]\", \"\").replace(\" [-1]\", \"\").replace(\"[+1]\", \"\").replace(\"[-1]\", \"\").replace(\"[A]\", \"\").replace(\"[V]\", \"\").replace(\n",
    "                \"[T]\", \"\").replace(\",\", \"\") for s in [aspectListtuple[i][0]]][0]\n",
    "\n",
    "    aspectListtuple = [tuple(l) for l in aspectListtuple]\n",
    "    temp = []\n",
    "    for i in range(0, len(aspectListtuple)):\n",
    "        if list(aspectListtuple[i])[0] != \"\":\n",
    "            temp += [tuple(aspectListtuple[i])]\n",
    "    aspectListtuple = temp\n",
    "    temp =[]\n",
    "    count_dict = collections.Counter([x for (x,y) in aspectListtuple])\n",
    "    for key, countii in count_dict.items():\n",
    "        if countii > 1:\n",
    "            temp += [(key, countii)]\n",
    "    count_dict = temp\n",
    "    temp = []\n",
    "    for aspect,c in aspectListtuple:\n",
    "        if aspect in [item for sublist in [list(elem) for elem in count_dict] for item in sublist]:\n",
    "            temp += [(aspect,c)]\n",
    "    aspectListtuple = temp\n",
    "    # print(aspectListtuple)\n",
    "\n",
    "    # print(aspectList)\n",
    "    aspectList2 = []\n",
    "    for i in range(0, len(aspectList)):\n",
    "        if ' [' in aspectList[i]:\n",
    "            temp1 = aspectList[i][aspectList[i].index(\" [\"):]\n",
    "            temp2 = aspectList[i][:aspectList[i].index(\" [\")]\n",
    "            aspectList2.append(temp1)\n",
    "            aspectList2.append(temp2)\n",
    "        else:\n",
    "            aspectList2.append(aspectList[i])\n",
    "\n",
    "    aspectList2 = [\n",
    "        s.replace(\" [\", \"[\").replace(\"[+1]\", \"\").replace(\"[-1]\", \"\").replace(\"[A]\", \"\").replace(\"[V]\", \"\").replace(\n",
    "            \"[T]\", \"\").replace(\",\", \"\") for s in aspectList2]\n",
    "    aspectList2 = list(filter(None, aspectList2))\n",
    "    aspectList = aspectList2\n",
    "    # Eliminating aspect which has 1 or less count\n",
    "    for aspect in aspectList:\n",
    "        if (aspectList.count(aspect) > 1):\n",
    "            if (outDict.keys() != aspect):\n",
    "                outDict[aspect] = aspectList.count(aspect)\n",
    "    outputAspect = sorted(outDict.items(), key=lambda x: x[1], reverse=True)\n",
    "    outputAspect = [i for i in outputAspect if i[0] != '##']\n",
    "    outputAspect = [i for i in outputAspect if len(i[0]) > 1]\n",
    "    \n",
    "    # print(outputAspect)\n",
    "    return outputAspect\n",
    "# --------------Aspect Extraction------------------\n",
    "\n",
    "\n",
    "# --------------Opinion Mining------------------\n",
    "def orientation(inputWord):\n",
    "    wordSynset = wordnet.synsets(inputWord)\n",
    "    if (len(wordSynset) != 0):\n",
    "        word = wordSynset[0].name()\n",
    "        orientation = sentiwordnet.senti_synset(word)\n",
    "        if (orientation.pos_score() > orientation.neg_score()):\n",
    "            return True\n",
    "        elif (orientation.pos_score() < orientation.neg_score()):\n",
    "            return False\n",
    "\n",
    "\n",
    "def OpinionMining(outputAspect, df_dict_list,n):\n",
    "    oAspectOpinionTuples = {}\n",
    "    JOpinionList = []\n",
    "    orientationCache = {}\n",
    "    negativeWordSet = {\"don't\", \"never\", \"nothing\", \"nowhere\", \"noone\", \"none\", \"not\",\n",
    "                       \"hasn't\", \"hadn't\", \"can't\", \"couldn't\", \"shouldn't\", \"won't\",\n",
    "                       \"wouldn't\", \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"ain't\"}\n",
    "\n",
    "    out = [\"\"]\n",
    "    for aspect, no in outputAspect:\n",
    "        aspectTokens = word_tokenize(aspect)\n",
    "        count = 0\n",
    "        for key, value in df_dict_list.items():\n",
    "            condition = True\n",
    "            isNegativeSen = False\n",
    "            for subWord in aspectTokens:\n",
    "                if (subWord in str(value).upper()):\n",
    "                    condition = condition and True\n",
    "                else:\n",
    "                    condition = condition and False\n",
    "\n",
    "            if (condition):\n",
    "                for negWord in negativeWordSet:\n",
    "                    if (not isNegativeSen):\n",
    "                        if negWord.upper() in str(value).upper():\n",
    "                            isNegativeSen = isNegativeSen or True\n",
    "\n",
    "                oAspectOpinionTuples.setdefault(aspect, [0, 0, 0])\n",
    "\n",
    "                for word, tag in value:\n",
    "\n",
    "                    if (tag == 'JJ' or tag == 'JJR' or tag == 'JJS' or tag == 'RB' or tag == 'RBR' or tag == 'RBS'):\n",
    "                        count += 1\n",
    "                        if (word not in orientationCache):\n",
    "                            orien = orientation(word)\n",
    "                            orientationCache[word] = orien\n",
    "                        else:\n",
    "                            orien = orientationCache[word]\n",
    "                        if (isNegativeSen and orien is not None):\n",
    "                            orien = not orien\n",
    "                        if (orien == True):\n",
    "                            oAspectOpinionTuples[aspect][0] += 1\n",
    "                        elif (orien == False):\n",
    "                            oAspectOpinionTuples[aspect][1] += 1\n",
    "                        elif (orien is None):\n",
    "                            oAspectOpinionTuples[aspect][2] += 1\n",
    "                        \n",
    "                        JOpinionList = JOpinionList + [(key, orien)]\n",
    "        JOpinionList_ = []\n",
    "        for i in range(0, len(JOpinionList)):\n",
    "            if i != len(JOpinionList) - 1:\n",
    "                if JOpinionList[i][0] != JOpinionList[i + 1][0]:\n",
    "                    JOpinionList_ = JOpinionList_ + [JOpinionList[i]]\n",
    "            else:\n",
    "                JOpinionList_ = JOpinionList_ + [JOpinionList[i]]\n",
    "        positives = []\n",
    "        negatives = []\n",
    "        Neutral = []\n",
    "        pcount = 0\n",
    "        necount = 0\n",
    "        nucount = 0\n",
    "        for aspect, no in outputAspect:\n",
    "            for key, Sentiment in JOpinionList_:\n",
    "                # And JOpinionList_ key is true\n",
    "                if ' '.join([str(elem) for elem in df_dict_clean_list.get(key)]).find(aspect.lower()) >= 0:\n",
    "                    if Sentiment == True:\n",
    "                        pcount += 1\n",
    "                    if Sentiment == False:\n",
    "                        necount += 1\n",
    "                    if Sentiment == None:\n",
    "                        nucount += 1\n",
    "            positives = positives + [(aspect, pcount)]\n",
    "            negatives = negatives + [(aspect, necount)]\n",
    "            Neutral = Neutral + [(aspect, nucount)]\n",
    "            pcount = 0\n",
    "            necount = 0\n",
    "            nucount = 0\n",
    "        aspect_reviews = []\n",
    "        for i in range(0, len(positives)):\n",
    "            aspect_reviews = aspect_reviews + [list(positives[i]) + [negatives[i][1]] + [Neutral[i][1]]]\n",
    "        aspect_reviews_c = []\n",
    "        for i in range(0, len(aspect_reviews)):\n",
    "            if aspect_reviews[i][0].lower() not in cachedStopWords and len(aspect_reviews[i][0].lower()) > 1:\n",
    "                aspect_reviews_c = aspect_reviews_c + [aspect_reviews[i]]\n",
    "        saspect_reviews = sorted(aspect_reviews_c, key=lambda x: x[1], reverse=True)[:5]\n",
    "        print('Product: ' + str(product_list[n]))\n",
    "        print('From path: ' + str(path_product_list[n][17:]))\n",
    "        for i in range(0, len(saspect_reviews)):\n",
    "            print(saspect_reviews[i][0], ':\\t\\tPositive ==> ', saspect_reviews[i][1], ':\\t\\tNegative ==> ',\n",
    "                  saspect_reviews[i][2])\n",
    "        return\n",
    "# --------------Opinion Mining------------------\n",
    "\n",
    "\n",
    "for n in range(0, len(path_product_list)):\n",
    "    [rev1, text_string] = setup(n)\n",
    "    cachedStopWords = PreProcess()\n",
    "    processed_list = process(text_string)\n",
    "    [df_dict_clean_list, df_dict_list] = posTagging(rev1)\n",
    "    outputAspect = AspectExtraction(df_dict_list)\n",
    "    TopFiveReviews = OpinionMining(outputAspect, df_dict_list,n)\n",
    "    # print(TopFiveReviews)\n",
    "#     if n == 0:\n",
    "#         metrics = metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a baseline was established above, we will explore the supervised learning approach. Convert tages to either positive or negative by making use of + and - signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index                                     Text_from_dict  Plus_counts  \\\n",
      "0      0   I purchased this monitor because of budgetary...            0   \n",
      "1      1  inexpensive [+1] [a] This item was the most in...            1   \n",
      "2      2  monitor [-1] My overall experience with this m...            0   \n",
      "3      3  screen [-1] , picture quality [-1] When the sc...            0   \n",
      "4      4  monitor [-1] , picture quality [-1] I 've view...            0   \n",
      "\n",
      "   Minus_counts  joined_signs  \n",
      "0             0             0  \n",
      "1             0             1  \n",
      "2            -1            -1  \n",
      "3            -2            -2  \n",
      "4            -2            -2  \n"
     ]
    }
   ],
   "source": [
    "[rev1, text_string] = setup(0)\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def Supervised():\n",
    "    df_dict = df[\"text\"].to_dict()\n",
    "    df_dict_clean_list = list(df_dict.items())\n",
    "    df_dict_clean_list = [(i, j.replace('[', ' [').replace('],', '] ,').replace('##', '').replace('][', '] [').replace('  ', ' ')) for i, j\n",
    "                          in df_dict_clean_list]\n",
    "    df_dict_clean_list = [list(elem) for elem in df_dict_clean_list]\n",
    "    df_dict_clean_list = [tuple(l) for l in df_dict_clean_list]\n",
    "    df_dict_clean_list = dict(df_dict_clean_list)\n",
    "    global df_clean_list\n",
    "    df_clean_list = pd.DataFrame(df_dict_clean_list.items(), columns=['Index','Text_from_dict'])\n",
    "    df_clean_list['Plus_counts'] = df_clean_list[['Text_from_dict']].applymap(lambda x: str.count(x, '+'))\n",
    "    df_clean_list['Minus_counts'] = df_clean_list[['Text_from_dict']].applymap(lambda x: str.count(x, '-'))*-1\n",
    "    joined_pos_neg = df_clean_list['Plus_counts'] + df_clean_list['Minus_counts']\n",
    "    df_clean_list['joined_signs'] = joined_pos_neg\n",
    "\n",
    "    df_dict = df[\"text\"].to_dict()\n",
    "    df_dict_clean_list = list(df_dict.items())\n",
    "    df_dict_clean_list = [\n",
    "        (i, j.replace('[', ' [').replace('],', '] ,').replace('##', '').replace('][', '] [').replace('  ', ' ')) for\n",
    "        i, j\n",
    "        in df_dict_clean_list]\n",
    "    df_dict_clean_list = [list(elem) for elem in df_dict_clean_list]\n",
    "    return [rev1, text_string, df_clean_list, df_dict_clean_list]\n",
    "supervised = Supervised()\n",
    "print(df_clean_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a different approach to POS tagging and use stemming in addition to the previous lemmatizing and tokenizing done earlier and more ways to clean data. Because it is supervised approach, the data needs to be treated in a different way. I used a similar approach I had taken in a different assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe word\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Clean Data\n",
    "def clean_preProcess_text(text):\n",
    "    global clean_dict\n",
    "    clean_dict = {}\n",
    "    for i in range(0, len(text)):\n",
    "        # make text into lower case\n",
    "        sentence = text.lower()\n",
    "        # tokenize text & take out punctuations\n",
    "        sentence = [word_cr.strip(string.punctuation) for word_cr in sentence.split(\" \")]\n",
    "        # Take out any word containing numbers\n",
    "        sentence = [word_cr for word_cr in sentence if not any(cr.isdigit() for cr in word_cr)]\n",
    "        # Take out any stop words\n",
    "        # stop = stopwords.words('english')\n",
    "        sentence = [x_cr for x_cr in sentence if x_cr not in stop]\n",
    "        # Take out any blank tokens\n",
    "        sentence = [t_cr for t_cr in sentence if len(t_cr) > 0]\n",
    "        # pos_tag sentence\n",
    "        pos_tags = pos_tag(sentence)\n",
    "        # Lemmatize sentence this deals with treating the variations of the same word\n",
    "        sentence = [WordNetLemmatizer().lemmatize(t_cr[0], get_wordnet_pos(t_cr[1])) for t_cr in pos_tags]\n",
    "        # Take out any word that is just one letter\n",
    "        sentence = [t_cr for t_cr in sentence if len(t_cr) > 1]\n",
    "        # Combine everything\n",
    "        sentence = \" \".join(sentence)\n",
    "        text = sentence\n",
    "        clean_dict[i] = sentence\n",
    "\n",
    "\n",
    "#Do POS tagging\n",
    "def posTagging2(rev1):\n",
    "    df_dict = rev1[\"Text\"].to_dict()\n",
    "    df_dict_clean_list = list(df_dict.items())\n",
    "    df_dict_clean_list = [(i, j.replace('[', ' [').replace('##', '').replace('][', '] [').replace('  ', ' ')) for i, j\n",
    "                          in df_dict_clean_list]\n",
    "    df_dict_clean_list = [list(elem) for elem in df_dict_clean_list]\n",
    "    for i in range(0, len(df_dict_clean_list)):\n",
    "        df_dict_clean_list[i][1] = process(df_dict_clean_list[i][1])\n",
    "        df_dict_clean_list[i][1] = pos_tag(df_dict_clean_list[i][1])\n",
    "    df_dict_clean_list = [tuple(l) for l in df_dict_clean_list]\n",
    "    df_dict_clean_list = dict(df_dict_clean_list)\n",
    "\n",
    "    stemmed_dict = {}\n",
    "    for ins in range(0,len(df_dict_clean_list)):\n",
    "        temp_str = ''\n",
    "        for i in range(0, len([list(ele) for ele in list(list(df_dict_clean_list.items())[ins])[1]])):\n",
    "            temp = [list(ele) for ele in list(list(df_dict_clean_list.items())[ins])[1]]\n",
    "            temp[i][0] = WordNetLemmatizer().lemmatize(temp[i][0])\n",
    "            temp[i][0] = PorterStemmer().stem(temp[i][0])\n",
    "            temp_str = temp_str + \" \" + temp[i][0]\n",
    "        stemmed_dict[ins] = temp_str\n",
    "    # print(list(df_dict_clean_list.items())[:2])\n",
    "    global df_stem\n",
    "    df_stem = pd.DataFrame(stemmed_dict.items(), columns=['Index','Stem_Text_from_dict'])\n",
    "    return [df_dict_clean_list, df_dict_list]\n",
    "\n",
    "[df_dict_clean_list, df_dict_list] = posTagging2(rev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional columns to Features to be able to obtain better predictions. 1) Add SentimentAnalyzer() polarity scores column. 2) Add number of charachters in sentences and number of words. 3) Convert text to vectors. 4) Apply tf-idf. 5) Add sings such as positives and negatives to have as labels. 6) Try different groupings and choose what fits best, Originally grouped labels with above 1 being very positive with label 2 and below 1 being very negative with label -2 but it had lower metrics so went with 3 classes of labels = Neutral, Positive, Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Plus_counts  Minus_counts    neg    neu    pos  compound  nb_chars_cr  \\\n",
      "0            0             0  0.000  1.000  0.000    0.0000           57   \n",
      "1            1             0  0.000  1.000  0.000    0.0000          121   \n",
      "2            0            -1  0.508  0.492  0.000   -0.4767           68   \n",
      "3            0            -2  0.250  0.565  0.185   -0.2023          130   \n",
      "4            0            -2  0.162  0.838  0.000   -0.4767          190   \n",
      "\n",
      "   nb_words_cr  doc2vec_vector_0  doc2vec_vector_1  ...  word_turn  word_use  \\\n",
      "0           10          0.113804         -0.032708  ...        0.0       0.0   \n",
      "1           23          0.009049          0.160494  ...        0.0       0.0   \n",
      "2           12         -0.042634          0.119056  ...        0.0       0.0   \n",
      "3           23          0.052556          0.226337  ...        0.0       0.0   \n",
      "4           35          0.099312          0.251304  ...        0.0       0.0   \n",
      "\n",
      "    word_ve  word_view  word_want  word_well  word_work  word_would  \\\n",
      "0  0.000000   0.000000        0.0        0.0        0.0         0.0   \n",
      "1  0.000000   0.000000        0.0        0.0        0.0         0.0   \n",
      "2  0.000000   0.000000        0.0        0.0        0.0         0.0   \n",
      "3  0.000000   0.000000        0.0        0.0        0.0         0.0   \n",
      "4  0.701588   0.350794        0.0        0.0        0.0         0.0   \n",
      "\n",
      "   joined_signs  Groups  \n",
      "0             0       0  \n",
      "1             1       1  \n",
      "2            -1      -1  \n",
      "3            -2      -2  \n",
      "4            -2      -2  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "df_clean_list[\"Text_stem_clean\"] = df_stem[\"Stem_Text_from_dict\"]#.apply(lambda x: clean_preProcess_text(x))\n",
    "# df_dict_list = clean_dict\n",
    "# First part of appending columns for sentiment analysis\n",
    "Text_sentiment = SentimentIntensityAnalyzer()\n",
    "df_clean_list[\"sentiments\"] = df_clean_list[\"Text_stem_clean\"].apply(lambda x: Text_sentiment.polarity_scores(x))\n",
    "df_clean_list = pd.concat([df_clean_list.drop(['sentiments'], axis=1), df_clean_list['sentiments'].apply(pd.Series)], axis=1)\n",
    "# append column for # of characters\n",
    "df_clean_list[\"nb_chars_cr\"] = df_clean_list[\"Text_from_dict\"].apply(lambda x: len(x))\n",
    "# Append column for # of words\n",
    "df_clean_list[\"nb_words_cr\"] = df_clean_list[\"Text_from_dict\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# Second part of appending columns\n",
    "# generate the columns for the \"doc2vec vector columns\"\n",
    "documents = [TaggedDocument(doc, [i_cd]) for i_cd, doc in enumerate(df_clean_list[\"Text_stem_clean\"].apply(lambda x_cr: x_cr.split(\" \")))]\n",
    "# train the model of Doc2Vec using text data\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "# Convert every document to a vector data\n",
    "doc2vec_df = df_clean_list[\"Text_stem_clean\"].apply(lambda x_cr: model.infer_vector(x_cr.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x_cr) for x_cr in doc2vec_df.columns]\n",
    "df_clean_list = pd.concat([df_clean_list, doc2vec_df], axis=1)\n",
    "# append columns of tf-idfs\n",
    "tfidf_cd = TfidfVectorizer(min_df=10)\n",
    "tfidf_result_cd = tfidf_cd.fit_transform(df_clean_list[\"Text_stem_clean\"]).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_result_cd, columns=tfidf_cd.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x_cd) for x_cd in tfidf_df.columns]\n",
    "tfidf_df.index = df_clean_list.index\n",
    "df_clean_list = pd.concat([df_clean_list, tfidf_df], axis=1)\n",
    "cols = df_clean_list.columns.tolist()\n",
    "cols = cols[2:4] + cols[6:] + cols[4:5]\n",
    "df_to_numbers = df_clean_list[cols]\n",
    "\n",
    "# print(df_clean_list)\n",
    "\n",
    "df_to_numbers = df_to_numbers.assign(Groups=df_to_numbers['joined_signs'])\n",
    "print(df_to_numbers.head())\n",
    "# del df_to_numbers['joined_signs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the classes are determined. Decided to go with 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is Neutral 1 is Positive -1 is Negative\n",
      " 0    261\n",
      " 1    151\n",
      "-1    119\n",
      "Name: Groups, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_to_numbers['Groups'] = np.where((df_to_numbers.joined_signs > 0 ),1, df_to_numbers['Groups'])\n",
    "df_to_numbers['Groups'] = np.where((df_to_numbers.joined_signs < 0 ),-1, df_to_numbers['Groups'])\n",
    "print('0 is Neutral', '1 is Positive', '-1 is Negative')\n",
    "print(df_to_numbers.Groups.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign labels and features to use in a supervised learning classifier and apply random forest classifier to predict untagged data. Print confusion matrix and classification report to asses the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionM RF:\n",
      " [[ 9 12  4]\n",
      " [ 5 50  1]\n",
      " [ 2 12 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.56      0.36      0.44        25\n",
      "    Positive       0.68      0.89      0.77        56\n",
      "    Negative       0.71      0.46      0.56        26\n",
      "\n",
      "    accuracy                           0.66       107\n",
      "   macro avg       0.65      0.57      0.59       107\n",
      "weighted avg       0.66      0.66      0.64       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# select the features\n",
    "label = \"Groups\"\n",
    "features = df_to_numbers.columns.tolist()[3:-2]\n",
    "# Divide the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_to_numbers[features], df_to_numbers[label], test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "# train a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=10000, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "print(\"ConfusionM RF:\\n\", CM)\n",
    "print(classification_report(y_test, y_pred, target_names=['Neutral', 'Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our f1 score which is the best indicator along the side the confusion matrix shows a value of 69% which should be good enough to predict untagged data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict untagged data with the classifier that was just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONITOR :\t\tPositive ==>  32 :\t\tNegative ==>  20\n",
      "PRICE :\t\tPositive ==>  12 :\t\tNegative ==>  0\n",
      "COMPUTER :\t\tPositive ==>  9 :\t\tNegative ==>  7\n",
      "ACER :\t\tPositive ==>  7 :\t\tNegative ==>  1\n",
      "SCREEN :\t\tPositive ==>  6 :\t\tNegative ==>  3\n"
     ]
    }
   ],
   "source": [
    "[rev1, text_string] = setup(0)\n",
    "cachedStopWords = PreProcess()\n",
    "processed_list = process(text_string)\n",
    "[df_dict_clean_list, df_dict_list] = posTagging(rev1)\n",
    "outputAspect = AspectExtraction(df_dict_list)\n",
    "predicted_y = []\n",
    "#Predict each untagged sentence\n",
    "for i in range(0,df_to_numbers[features].shape[0]):\n",
    "    if df_to_numbers[label].iloc[i] != 0:\n",
    "        predicted_y += [df_to_numbers[label].iloc[i]]\n",
    "    else:\n",
    "        dfi = df_to_numbers[features].iloc[i].reset_index().T\n",
    "        dfi.columns = list(df_to_numbers[features])\n",
    "        dfi = dfi.drop(dfi.index[0])\n",
    "        y_pred_i = rf.predict(dfi)\n",
    "        predicted_y += [y_pred_i[0]]\n",
    "\n",
    "\n",
    "list_of_aspects = [i[0] for i in aspectListtuple]\n",
    "dict_count_of_aspect_positives = {}\n",
    "dict_count_of_aspect_negatives = {}\n",
    "for i in list_of_aspects:\n",
    "    dict_count_of_aspect_positives[i] = 0\n",
    "    dict_count_of_aspect_negatives[i] = 0\n",
    "# print(list_of_aspects)\n",
    "# Add positives and negatives\n",
    "for aspect,lineindex in aspectListtuple:\n",
    "    if predicted_y[lineindex] > 0:\n",
    "        dict_count_of_aspect_positives[aspect] = dict_count_of_aspect_positives[aspect] +1\n",
    "    if predicted_y[lineindex] < 0:\n",
    "        dict_count_of_aspect_negatives[aspect] = dict_count_of_aspect_negatives[aspect] + 1\n",
    "\n",
    "dict_count_of_aspect_positives = {k: v for k, v in sorted(dict_count_of_aspect_positives.items(), key=lambda item: item[1],reverse=True)}\n",
    "for i in range(0,5):\n",
    "    print(list(dict_count_of_aspect_positives.items())[i][0], ':\\t\\tPositive ==> ', list(dict_count_of_aspect_positives.\n",
    "                items())[i][1], ':\\t\\tNegative ==> ',list(dict_count_of_aspect_negatives.items())[i][1])\n",
    "# print(dict_count_of_aspect_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison can be made between the added sentiments a supervised learning approach can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional exploration into Apriori alogrithm\n",
    "Convert data into dataframe with most frequent n features and then run apriori algorithm to determine rules from the sets of aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RelationRecord(items=frozenset({'nan'}), support=0.9937888198757764, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'nan'}), confidence=0.9937888198757764, lift=1.0)]), RelationRecord(items=frozenset({'ACER', 'nan'}), support=0.16149068322981366, ordered_statistics=[OrderedStatistic(items_base=frozenset({'ACER'}), items_add=frozenset({'nan'}), confidence=0.9629629629629629, lift=0.9689814814814814)]), RelationRecord(items=frozenset({'nan', 'COMPUTER'}), support=0.13043478260869565, ordered_statistics=[OrderedStatistic(items_base=frozenset({'COMPUTER'}), items_add=frozenset({'nan'}), confidence=0.9545454545454545, lift=0.9605113636363636)]), RelationRecord(items=frozenset({'nan', 'MONITOR'}), support=0.4161490683229814, ordered_statistics=[OrderedStatistic(items_base=frozenset({'MONITOR'}), items_add=frozenset({'nan'}), confidence=0.9852941176470588, lift=0.9914522058823529)]), RelationRecord(items=frozenset({'PRICE', 'nan'}), support=0.17391304347826086, ordered_statistics=[OrderedStatistic(items_base=frozenset({'PRICE'}), items_add=frozenset({'nan'}), confidence=0.9655172413793103, lift=0.971551724137931)]), RelationRecord(items=frozenset({'SCREEN', 'nan'}), support=0.10559006211180125, ordered_statistics=[OrderedStatistic(items_base=frozenset({'SCREEN'}), items_add=frozenset({'nan'}), confidence=0.9444444444444444, lift=0.9503472222222222)]), RelationRecord(items=frozenset({'TIME', 'nan'}), support=0.13664596273291926, ordered_statistics=[OrderedStatistic(items_base=frozenset({'TIME'}), items_add=frozenset({'nan'}), confidence=0.9565217391304349, lift=0.9625000000000001)]), RelationRecord(items=frozenset({'TIME', 'nan', 'COMPUTER'}), support=0.024844720496894408, ordered_statistics=[OrderedStatistic(items_base=frozenset({'TIME', 'COMPUTER'}), items_add=frozenset({'nan'}), confidence=0.7999999999999999, lift=0.8049999999999999)]), RelationRecord(items=frozenset({'PRICE', 'nan', 'MONITOR'}), support=0.024844720496894408, ordered_statistics=[OrderedStatistic(items_base=frozenset({'PRICE', 'MONITOR'}), items_add=frozenset({'nan'}), confidence=0.7999999999999999, lift=0.8049999999999999)]), RelationRecord(items=frozenset({'SCREEN', 'nan', 'MONITOR'}), support=0.018633540372670808, ordered_statistics=[OrderedStatistic(items_base=frozenset({'SCREEN', 'MONITOR'}), items_add=frozenset({'nan'}), confidence=0.7500000000000001, lift=0.7546875000000001)]), RelationRecord(items=frozenset({'TIME', 'nan', 'MONITOR'}), support=0.018633540372670808, ordered_statistics=[OrderedStatistic(items_base=frozenset({'TIME', 'MONITOR'}), items_add=frozenset({'nan'}), confidence=0.7500000000000001, lift=0.7546875000000001)])]\n"
     ]
    }
   ],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "apriori_dict ={}\n",
    "apriori_list = []\n",
    "index = 0\n",
    "for i in df_dict_clean_list.items():\n",
    "    apriori_dict[index] = ' '.join([str(elem) for elem in [at[0] for at in i[1]]])\n",
    "    apriori_list += [tuple([at[0] for at in i[1]])]\n",
    "    index += 1\n",
    "apriori_list2 = []\n",
    "for i in apriori_list:\n",
    "    al2 = []\n",
    "    for j in i:\n",
    "        if j.upper() in list_of_aspects:\n",
    "            al2 += [j.upper()]\n",
    "    apriori_list2 += [tuple(al2)]\n",
    "\n",
    "columnsi = [item for item in list(set(list_of_aspects)) if len(item)>1 and '!' not in item and item not in set(stopwords.words('english')) and item != 'IM'][:-2]\n",
    "mylist = [item for item in list_of_aspects if len(item)>1 and '!' not in item and item not in set(stopwords.words('english')) and item != 'IM'][:-2]\n",
    "from collections import Counter\n",
    "c = Counter(mylist)\n",
    "cp = c.most_common(6)\n",
    "cp = [ap[0] for ap in cp]\n",
    "columnsi = cp\n",
    "\n",
    "real_df = pd.DataFrame('nan', index=list(range(0,len(rev1['Text']))), columns=columnsi, dtype = object)\n",
    "for i in aspectListtuple:\n",
    "    if i[0] in cp:\n",
    "        real_df.at[i[1], i[0]] = i[0]\n",
    "# real_df = real_df.replace(np.nan, 0)\n",
    "# real_df = real_df.replace('', np.nan)\n",
    "real_list = real_df.values.tolist()\n",
    "real_list.insert(0,real_df.columns.values.tolist())\n",
    "apriori_real_list = []\n",
    "for i in real_list:\n",
    "    if i != ['nan','nan','nan','nan','nan','nan']:\n",
    "        apriori_real_list += [i]\n",
    "association_results = list(apriori(apriori_real_list, min_support=0.01, min_confidence=0.7, min_lift=0.5, min_length=2))\n",
    "print(association_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is shown in this file since top 6 aspect counts are: ('MONITOR', 72), ('PRICE', 28), ('ACER', 26), ('TIME', 24), ('COMPUTER', 22), ('SCREEN', 17) these tags are not sufficiently spread between the tagged rows in order to create very good rules for this file specifically. The appriori algorithm may not be the approach to take for this file as the data is not fully spread out. Even when using a minimum support of 1% as suggested by Minqing Hu and Bing Liu: Mining and Summarizing Customer Reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
